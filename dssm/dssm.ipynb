{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from file_storage import FileStorage\n",
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import transliterate\n",
    "import tqdm\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_storage = FileStorage('../filtered_storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157154"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLITERATE_DICT = {\n",
    "    'а': 'a',\n",
    "    'б': 'b',\n",
    "    'в': 'v',\n",
    "    'г': 'g',\n",
    "    'д': 'd',\n",
    "    'е': 'e',\n",
    "    'ж': 'zh',\n",
    "    'з': 'z',\n",
    "    'и': 'i',\n",
    "    'к': 'k',\n",
    "    'л': 'l',\n",
    "    'м': 'm',\n",
    "    'н': 'n',\n",
    "    'о': 'o',\n",
    "    'п': 'p',\n",
    "    'р': 'r',\n",
    "    'с': 's',\n",
    "    'т': 't',\n",
    "    'ф': 'f',\n",
    "    'х': 'h',\n",
    "    'ц': 'ts',\n",
    "    'ч': 'ch',\n",
    "    'ш': 'sh',\n",
    "    'щ': 'sch',\n",
    "    'ъ': \"'\",\n",
    "    'ы': 'y',\n",
    "    'ь': \"'\",\n",
    "    'э': 'e',\n",
    "    'ю': 'ju',\n",
    "    'я': 'ya',\n",
    "    'π': 'pi',\n",
    "    'ı': 'i',\n",
    "    'ə': 'e',\n",
    "    'ل': 'j',\n",
    "    'ƒ': 'f',\n",
    "    'ﬁ': 'fi',\n",
    "    '\\xad': '-',\n",
    "    'µ': 'mu',\n",
    "    '\\u200b': ' ',\n",
    "    'ː': ':',\n",
    "    '—': '-',\n",
    "    '−': '-',\n",
    "    '–': '-',\n",
    "    '”': '\"',\n",
    "    '“': '\"',\n",
    "    '«': '\"',\n",
    "    '»': '\"',\n",
    "    'у': 'y',\n",
    "    '’': '\"',\n",
    "    '‘': '\"',\n",
    "    '`': \"'\",\n",
    "    '„': '\"',\n",
    "    '·': ',',\n",
    "    '•': ',',\n",
    "    '…': ' ',\n",
    "    # https://www.redhat.com/archives/fedora-extras-commits/2007-June/msg03617.html\n",
    "    \"\\u0621\": \"'\", # hamza-on-the-line\n",
    "    \"\\u0622\": \"|\", # madda\n",
    "    \"\\u0623\": \">\", # hamza-on-'alif\n",
    "    \"\\u0624\": \"&\", # hamza-on-waaw\n",
    "    \"\\u0625\": \"<\", # hamza-under-'alif\n",
    "    \"\\u0626\": \"}\", # hamza-on-yaa'\n",
    "    \"\\u0627\": \"A\", # bare 'alif\n",
    "    \"\\u0628\": \"b\", # baa'\n",
    "    \"\\u0629\": \"p\", # taa' marbuuTa\n",
    "    \"\\u062A\": \"t\", # taa'\n",
    "    \"\\u062B\": \"v\", # thaa'\n",
    "    \"\\u062C\": \"j\", # jiim\n",
    "    \"\\u062D\": \"H\", # Haa'\n",
    "    \"\\u062E\": \"x\", # khaa'\n",
    "    \"\\u062F\": \"d\", # daal\n",
    "    \"\\u0630\": \"*\", # dhaal\n",
    "    \"\\u0631\": \"r\", # raa'\n",
    "    \"\\u0632\": \"z\", # zaay\n",
    "    \"\\u0633\": \"s\", # siin\n",
    "    \"\\u0634\": \"$\", # shiin\n",
    "    \"\\u0635\": \"S\", # Saad\n",
    "    \"\\u0636\": \"D\", # Daad\n",
    "    \"\\u0637\": \"T\", # Taa'\n",
    "    \"\\u0638\": \"Z\", # Zaa' (DHaa')\n",
    "    \"\\u0639\": \"E\", # cayn\n",
    "    \"\\u063A\": \"g\", # ghayn\n",
    "    \"\\u0640\": \"_\", # taTwiil\n",
    "    \"\\u0641\": \"f\", # faa'\n",
    "    \"\\u0642\": \"q\", # qaaf\n",
    "    \"\\u0643\": \"k\", # kaaf\n",
    "    \"\\u0644\": \"l\", # laam\n",
    "    \"\\u0645\": \"m\", # miim\n",
    "    \"\\u0646\": \"n\", # nuun\n",
    "    \"\\u0647\": \"h\", # haa'\n",
    "    \"\\u0648\": \"w\", # waaw\n",
    "    \"\\u0649\": \"Y\", # 'alif maqSuura\n",
    "    \"\\u064A\": \"y\", # yaa'\n",
    "    \"\\u064B\": \"F\", # fatHatayn\n",
    "    \"\\u064C\": \"N\", # Dammatayn\n",
    "    \"\\u064D\": \"K\", # kasratayn\n",
    "    \"\\u064E\": \"a\", # fatHa\n",
    "    \"\\u064F\": \"u\", # Damma\n",
    "    \"\\u0650\": \"i\", # kasra\n",
    "    \"\\u0651\": \"~\", # shaddah\n",
    "    \"\\u0652\": \"o\", # sukuun\n",
    "    \"\\u0670\": \"`\", # dagger 'alif\n",
    "    \"\\u0671\": \"{\", # waSlaﬁ\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_from_the_internet(url):\n",
    "    try:\n",
    "        return urlopen(url).read().decode('utf-8')\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except urllib.error.HTTPError as e:\n",
    "        code = e.code\n",
    "        if code != 404:\n",
    "            print(e)\n",
    "        return code\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34753821/remove-diacritics-from-string-for-search-function\n",
    "\n",
    "def shave_marks(txt):\n",
    "    \"\"\"This method removes all diacritic marks from the given string\"\"\"\n",
    "    norm_txt = unicodedata.normalize('NFD', txt)\n",
    "    shaved = ''.join(c for c in norm_txt if not unicodedata.combining(c))\n",
    "    return unicodedata.normalize('NFC', shaved)\n",
    "\n",
    "\n",
    "def is_english_letters(string):\n",
    "    return re.search(r'[^a-zA-Z0-9°_©®™;§,№!#@.×:+=()/£¥€$|<>~{}\\\\\\[\\]%&*^?\"\\'-]', string) is None\n",
    "\n",
    "\n",
    "def try_transliterate(query):\n",
    "    query = unicodedata.normalize('NFC', shave_marks(query).lower())\n",
    "    try:\n",
    "        return transliterate.translit(query, reversed=True)\n",
    "    except transliterate.exceptions.LanguageDetectionError as query_error:\n",
    "        transliteration = []\n",
    "        for word in query.split():\n",
    "            if is_english_letters(word):\n",
    "                transliteration.append(word)\n",
    "            else:\n",
    "                try:\n",
    "                    transliteration.append(transliterate.translit(word, reversed=True))\n",
    "                except transliterate.exceptions.LanguageDetectionError as e:\n",
    "                    new_word = []\n",
    "                    for ch in word:\n",
    "                        translited_ch = TRANSLITERATE_DICT.get(ch, ch)\n",
    "                        new_word.append(translited_ch)\n",
    "                    transliteration.append(''.join(new_word))\n",
    "        return ' '.join(transliteration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50786it [00:06, 9098.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b03dc7fde968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtransliteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_transliterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtransliterated_learn_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransliteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl_end\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-c523da0d854d>\u001b[0m in \u001b[0;36mtry_transliterate\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NFC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshave_marks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransliterate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtransliterate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLanguageDetectionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mquery_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtransliteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transliterate/utils.py\u001b[0m in \u001b[0;36mtranslit\u001b[0;34m(value, language_code, reversed, strict)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlanguage_code\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mlanguage_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfail_silently\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transliterate/utils.py\u001b[0m in \u001b[0;36mdetect_language\u001b[0;34m(text, num_words, fail_silently, heavy_check)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mletter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlanguage_pack\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable_language_packs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mlanguage_pack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlanguage_pack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                     \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage_pack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_code\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transliterate/base.py\u001b[0m in \u001b[0;36mcontains\u001b[0;34m(cls, character)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mchar_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NFC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mchar_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mcharacter_range\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharacter_ranges\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m                 \u001b[0mrange_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcharacter_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0mrange_upper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcharacter_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('req_ans_learn.tsv', encoding='utf-8-sig') as train_file, open('transliterated_learn.tsv', 'w') as transliterated_learn_file:\n",
    "    for line in tqdm.tqdm(train_file):\n",
    "        query, url_end = line.strip().split('\\t')\n",
    "        transliteration = try_transliterate(query)\n",
    "        transliterated_learn_file.write(transliteration + '\\t' + url_end + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.quora.com/How-can-I-extract-only-text-data-from-HTML-pages\n",
    "\n",
    "def informative(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    elif len(element) < 10:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_text(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    data = soup.findAll(text=True)\n",
    "\n",
    "    text = []\n",
    "    informative_lines = [line for line in data if informative(line)]\n",
    "    started = False\n",
    "    for line in data:\n",
    "        if line.strip() == 'Jump to search':\n",
    "            started = True\n",
    "            continue\n",
    "        if started and informative(line):\n",
    "            if re.search(r'Cached time: \\d+\\nCache expiry: \\d+', line) is not None:\n",
    "                break\n",
    "            text.append(line.strip())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGINNING = 'https://simple.wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text(file_storage.read(BEGINNING + '/wiki/Germany'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(string):\n",
    "    return [word for word in re.split('[,.!?;:() \\t\\n]', str(string).lower()) if word]\n",
    "\n",
    "\n",
    "def get_ngrams(n, word_list):\n",
    "    if n == 1:\n",
    "        return ''.join(word_list)\n",
    "    else:\n",
    "        return [\n",
    "            word[i:i + n]\n",
    "            for word in word_list\n",
    "            for i in range(len(word) - n + 1)\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_n_gram_counter(n, collection):\n",
    "    counter = Counter()\n",
    "    for element in collection:\n",
    "        counter.update(get_ngrams(n, get_words(element)))\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    def __init__(self, elements):\n",
    "        self._ind_to_elem = elements\n",
    "        self._elem_to_ind = {elem: ind for ind, elem in enumerate(elements)}\n",
    "        \n",
    "    def get_elem(self, ind):\n",
    "        return self._ind_to_elem[ind]\n",
    "    \n",
    "    def get_ind(self, elem, default=None):\n",
    "        return self._elem_to_ind.get(elem, default)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._ind_to_elem)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_counter(cls, counter, n_most_common):\n",
    "        return Storage([elem for elem, _ in counter.most_common(n_most_common)])\n",
    "    \n",
    "\n",
    "class Encoder:\n",
    "    def encode(self, example):\n",
    "        raise NotImplemented()\n",
    "\n",
    "    def encode_with_padding(self, examples):\n",
    "        codes = [self.encode(example) for example in examples]\n",
    "        max_len = max(map(len, codes))\n",
    "        return np.array([\n",
    "            np.concatenate([code, np.zeros(max_len - len(code))])\n",
    "            for code in codes\n",
    "        ], dtype=np.int32)\n",
    "\n",
    "\n",
    "class BagOfNgramsEncoder(Encoder):\n",
    "    def __init__(self, collection, ngram_number_array):\n",
    "        self._ngram_storages = [\n",
    "            Storage.from_counter(get_n_gram_counter(n + 1, collection), ngram_number)\n",
    "            for n, ngram_number in enumerate(ngram_number_array)\n",
    "        ]\n",
    "        self._code_size = sum(map(len, self._ngram_storages)) + 2\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return len(self._ngram_storages)\n",
    "    \n",
    "    def _encode_word(self, word):\n",
    "        code = []\n",
    "        if len(word) > 2:\n",
    "            for i in range(len(word) - self.n + 1):\n",
    "                ind = 1\n",
    "                for ngram_len in reversed(range(1, self.n + 1)):\n",
    "                    ngram = word[i:i + ngram_len]\n",
    "                    ngram_ind = self._ngram_storages[ngram_len - 1].get_ind(ngram)\n",
    "                    if ngram_ind is None:\n",
    "                        ind += len(self._ngram_storages[ngram_len - 1])\n",
    "                    else:\n",
    "                        ind += ngram_ind\n",
    "                        code.append(ind)\n",
    "                        break\n",
    "                else:\n",
    "                    code.append(ind)\n",
    "        elif len(word) == 2:\n",
    "            bigram_ind = self._ngram_storages[1].get_ind(word)\n",
    "            if bigram_ind is not None:\n",
    "                return [bigram_ind]\n",
    "            else:\n",
    "                return [\n",
    "                    self._ngram_storages[0].get_ind(word[0], self._code_size - 1),\n",
    "                    self._ngram_storages[0].get_ind(word[1], self._code_size - 1),\n",
    "                ]\n",
    "        elif len(word) == 1:\n",
    "            return [self._ngram_storages[0].get_ind(word, self._code_size - 1)]\n",
    "        return code\n",
    "\n",
    "    def encode(self, string):\n",
    "        return sum((self._encode_word(word) for word in get_words(string)), [])\n",
    "        \n",
    "    @property\n",
    "    def code_size(self):\n",
    "        return self._code_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocEncoder(Encoder):    \n",
    "    def __init__(self, storage, collection, n_words, n_trigrams):\n",
    "        trigram_counter = Counter()\n",
    "        word_counter = Counter()\n",
    "        \n",
    "        for example in tqdm.tqdm(set(collection)):\n",
    "            html = storage.read(BEGINNING + example)\n",
    "            if html is None:\n",
    "                continue\n",
    "            text = get_text(html)\n",
    "            words, trigrams = self._prepare_example(text)\n",
    "            word_counter.update(words)\n",
    "            trigram_counter.update(trigrams)\n",
    "\n",
    "        self._word_storage = Storage.from_counter(word_counter, n_words)\n",
    "        self._trigram_storage = Storage.from_counter(trigram_counter, n_trigrams)    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _prepare_example(example):\n",
    "        words = []\n",
    "        trigrams = []\n",
    "        for example_part in example:\n",
    "            curr_words = get_words(example_part)\n",
    "            words += curr_words\n",
    "            trigrams += get_ngrams(3, curr_words)\n",
    "        return words, trigrams\n",
    "    \n",
    "    def _encode_word(self, word):\n",
    "        word_code = self._word_storage.get_ind(word)\n",
    "        if word_code is None:\n",
    "            return [\n",
    "                1 + len(self._word_storage) +\n",
    "                self._trigram_storage.get_ind(trigram, self._unk_ind - 1 - len(self._word_storage))\n",
    "                for trigram in get_ngrams(3, [word])\n",
    "            ]\n",
    "        else:\n",
    "            return [word_code + 1]\n",
    "    \n",
    "    def encode(self, example):\n",
    "        return sum(\n",
    "            (\n",
    "                self._encode_word(word)\n",
    "                for example_part in example\n",
    "                for word in get_words(example_part)\n",
    "            ), []\n",
    "        )\n",
    "                 \n",
    "    @property\n",
    "    def _unk_ind(self):\n",
    "        return self.code_size - 1\n",
    "                   \n",
    "    @property\n",
    "    def _pad_ind(self):\n",
    "        return 0\n",
    "                                                  \n",
    "    @property\n",
    "    def code_size(self):\n",
    "        return len(self._word_storage) + len(self._trigram_storage) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeWithSampling(Encoder):\n",
    "    def __init__(self, codes_storage, doc_encode_len, code_size):\n",
    "        self._storage = codes_storage\n",
    "        self._doc_encode_len = doc_encode_len\n",
    "        self._code_size = code_size\n",
    "        \n",
    "    def encode(self, doc_name):\n",
    "        code = self._storage.read(doc_name).split(',')\n",
    "        if len(code) > self._doc_encode_len:\n",
    "            ind = np.random.randint(len(code) - self._doc_encode_len + 1)\n",
    "            code = code[ind:ind + self._doc_encode_len]\n",
    "        return list(map(int, code))\n",
    "    \n",
    "    @property\n",
    "    def code_size(self):\n",
    "        return self._code_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    data = pd.read_csv(path, sep='\\t', header=None).values\n",
    "    return data[:, 0], data[:, 1]\n",
    "\n",
    "\n",
    "def dump(path, *arrays):\n",
    "    data_frame = pd.DataFrame(data=np.transpose(np.array(arrays)))\n",
    "    data_frame.to_csv(path, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "queries, docs = read_data('transliterated_learn.tsv')\n",
    "queries_train, queries_val, docs_train, docs_val = train_test_split(queries, docs, test_size=0.05, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump('train.tsv', queries_train, docs_train)\n",
    "dump('val.tsv', queries_val, docs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train, docs_train = read_data('train.tsv')\n",
    "queries_val, docs_val = read_data('val.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_encoder = BagOfNgramsEncoder(queries_train, [600, 2500, 20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word_num = 65000\n",
    "doc_trigram_num = 10000\n",
    "doc_code_size = doc_word_num + doc_trigram_num + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_encoder = DocEncoder(file_storage, docs_train, doc_word_num, doc_trigram_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('big_big_doc_encoder.pkl', 'wb') as doc_encoder_file:\n",
    "    pickle.dump(doc_encoder, doc_encoder_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('big_big_doc_encoder.pkl', 'rb') as doc_encoder_file:\n",
    "    doc_encoder = pickle.load(doc_encoder_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_encoder.code_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = FileStorage('encoded_docs', need_compression=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in tqdm.tqdm(set(np.concatenate([docs_train, docs_val]))):\n",
    "    if doc not in encoded_docs and BEGINNING + doc in file_storage:\n",
    "        code = doc_encoder.encode(get_text(file_storage.read(BEGINNING + doc)))\n",
    "        encoded_docs.write(doc, ','.join(map(str, code)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первые попытки были были не очень успешными. Из-за длинных документов все довольно долго училось, поэтому в последствии я переделал на bag of words для документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiled(x_positive, y_positive, y_negative, batch_size, negative_cnt):\n",
    "    return [\n",
    "        np.tile(x_positive, (1 + negative_cnt, 1)),\n",
    "        np.concatenate([y_positive, np.repeat(y_negative, batch_size, axis=0)], axis=0),\n",
    "    ]\n",
    "\n",
    "\n",
    "def batch_generator(queries, doc_names, doc_encoder, query_encoder, batch_size, negative_cnt=2):\n",
    "    allowed_inds = [ind for ind, name in enumerate(doc_names) if name in doc_encoder._storage]\n",
    "    while True:\n",
    "        try:\n",
    "            indexes = np.random.choice(allowed_inds, batch_size + negative_cnt, replace=False)\n",
    "            batch_indexes = indexes[:batch_size]\n",
    "            query_codes = query_encoder.encode_with_padding(queries[indexes])\n",
    "            positive_query_codes = query_codes[:batch_size]\n",
    "            negative_query_codes = query_codes[batch_size:]\n",
    "            doc_codes = doc_encoder.encode_with_padding(doc_names[batch_indexes])\n",
    "            if len(np.where(doc_codes >= doc_encoder.code_size)[0]) == 0:\n",
    "                yield (\n",
    "                    get_tiled(doc_codes, positive_query_codes, negative_query_codes, batch_size, negative_cnt),\n",
    "                    np.concatenate([np.ones(batch_size), np.zeros(batch_size * negative_cnt)]),\n",
    "                )\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense(units, activation='relu'):\n",
    "    return keras.layers.Dense(\n",
    "        units, activation=activation, kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_embed(embed_layers, data):\n",
    "    for layer in embed_layers:\n",
    "        data = layer(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = keras.layers.Input(shape=(None,), dtype='int32')\n",
    "query = keras.layers.Input(shape=(None,), dtype='int32')\n",
    "\n",
    "doc_embed_layers = [\n",
    "    keras.layers.Embedding(doc_encoder_with_sampling.code_size, 128),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "query_embed_layers = [\n",
    "    keras.layers.Embedding(query_encoder.code_size, 128),\n",
    "    keras.layers.LSTM(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "doc_embed = get_embed(doc_embed_layers, doc)\n",
    "query_embed = get_embed(query_embed_layers, query)\n",
    "\n",
    "similarity = keras.layers.Dot(axes=1, normalize=True)([doc_embed, query_embed])\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[doc, query],\n",
    "    outputs=similarity,\n",
    ")\n",
    "\n",
    "model.compile(keras.optimizers.Adam(lr=1e-4), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "negative_cnt = 2\n",
    "\n",
    "training_history = model.fit_generator(\n",
    "    generator=batch_generator(\n",
    "        queries_train, docs_train, doc_encoder_with_sampling,\n",
    "        query_encoder, batch_size, negative_cnt\n",
    "    ),\n",
    "    epochs=50,\n",
    "    steps_per_epoch=1000,\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(batch_size=batch_size),\n",
    "        keras.callbacks.ModelCheckpoint('first_dssm.bin', monitor='val_loss', save_best_only=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=3, min_delta=0.0002),\n",
    "    ],\n",
    "    validation_data=batch_generator(\n",
    "        queries_val, docs_val, doc_encoder_with_sampling,\n",
    "        query_encoder, batch_size, negative_cnt\n",
    "    ),\n",
    "    validation_steps=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_loss(similarity, labels):\n",
    "    return 1 - keras.backend.mean(similarity * labels)\n",
    "\n",
    "\n",
    "def mean_negative_score(score, labels):\n",
    "    return keras.backend.sum(score * (1 - labels)) / keras.backend.sum(1 - labels)\n",
    "\n",
    "\n",
    "def mean_positive_score(score, labels):\n",
    "    return keras.backend.sum(score * labels) / keras.backend.sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_docs(encoded_docs, doc_code_size, doc_names):\n",
    "    doc_data = []\n",
    "    doc_col_indexes = []\n",
    "    doc_indptr = []\n",
    "    for doc in doc_names:            \n",
    "        doc_counter = Counter([\n",
    "            int(code) % doc_code_size\n",
    "            for code in encoded_docs.read(doc).split(',') if code\n",
    "        ])\n",
    "        doc_indptr.append(len(doc_col_indexes))\n",
    "        for token_code, cnt in doc_counter.items():\n",
    "            doc_data.append(cnt)\n",
    "            doc_col_indexes.append(token_code)\n",
    "    doc_indptr.append(len(doc_col_indexes))\n",
    "    return csr_matrix(\n",
    "        (doc_data, doc_col_indexes, doc_indptr), shape=(len(doc_names), doc_code_size)\n",
    "    )\n",
    "\n",
    "\n",
    "def sparse_docs_batch_generator(\n",
    "        queries, doc_names, encoded_docs, query_encoder, batch_size, doc_code_size):\n",
    "    allowed_inds = [ind for ind, name in enumerate(doc_names) if name in encoded_docs]\n",
    "    while True:\n",
    "        positive_cnt = np.random.randint(4, batch_size - 4)\n",
    "        negative_cnt = batch_size - positive_cnt\n",
    "        indexes = np.random.choice(allowed_inds, positive_cnt + 2 * negative_cnt, replace=False)\n",
    "        query_indexes = np.concatenate([indexes[:positive_cnt], indexes[batch_size:]])\n",
    "        query_codes = query_encoder.encode_with_padding(queries[query_indexes])\n",
    "\n",
    "        yield (\n",
    "            [get_sparse_docs(encoded_docs, doc_code_size, doc_names[indexes[:batch_size]]), query_codes],\n",
    "            np.concatenate([np.ones(positive_cnt), np.zeros(negative_cnt)]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingGenerator:\n",
    "    def __init__(\n",
    "            self, queries, doc_names, encoded_docs, query_encoder,\n",
    "            half_batch_size, doc_code_size, initial_negative_cnt):\n",
    "        self._queries = queries\n",
    "        self._doc_names = doc_names\n",
    "        self._encoded_docs = encoded_docs\n",
    "        self._query_encoder = query_encoder\n",
    "        self._half_batch_size = half_batch_size\n",
    "        self._doc_code_size = doc_code_size\n",
    "        self._initial_negative_cnt = initial_negative_cnt\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_to_score(similarity):\n",
    "    return (similarity + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "doc = keras.layers.Input(shape=(doc_code_size,), sparse=True)\n",
    "query = keras.layers.Input(shape=(None,), dtype='int32')\n",
    "\n",
    "doc_embed_layers = [\n",
    "    get_dense(256),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "query_embed_layers = [\n",
    "    keras.layers.Embedding(query_encoder.code_size, 128),\n",
    "    keras.layers.LSTM(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "doc_embed = get_embed(doc_embed_layers, doc)\n",
    "query_embed = get_embed(query_embed_layers, query)\n",
    "\n",
    "similarity = keras.layers.Dot(axes=1, normalize=True)([doc_embed, query_embed])\n",
    "score = keras.layers.Lambda(sim_to_score)(similarity)\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[doc, query],\n",
    "    outputs=score,\n",
    ")\n",
    "\n",
    "model.compile(keras.optimizers.Adam(), loss='binary_crossentropy', metrics=[\n",
    "    mean_positive_sim, mean_negative_sim,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 1.0618 - mean_positive_sim: -0.0261 - mean_negative_sim: 0.4858 - val_loss: 0.7051 - val_mean_positive_sim: -0.0880 - val_mean_negative_sim: 0.4885\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 332s 332ms/step - loss: 0.6985 - mean_positive_sim: -0.0198 - mean_negative_sim: 0.4878 - val_loss: 0.7030 - val_mean_positive_sim: -0.1475 - val_mean_negative_sim: 0.4903\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 336s 336ms/step - loss: 0.6957 - mean_positive_sim: -0.0211 - mean_negative_sim: 0.4914 - val_loss: 0.6930 - val_mean_positive_sim: -0.0082 - val_mean_negative_sim: 0.4731\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 292s 292ms/step - loss: 0.6930 - mean_positive_sim: -0.0512 - mean_negative_sim: 0.4738 - val_loss: 0.6929 - val_mean_positive_sim: -0.0074 - val_mean_negative_sim: 0.4808\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 331s 331ms/step - loss: 0.6929 - mean_positive_sim: -0.0459 - mean_negative_sim: 0.4763 - val_loss: 0.6933 - val_mean_positive_sim: -0.0608 - val_mean_negative_sim: 0.4856\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.6931 - mean_positive_sim: -0.0413 - mean_negative_sim: 0.4798 - val_loss: 0.6926 - val_mean_positive_sim: -0.0319 - val_mean_negative_sim: 0.4834\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 302s 302ms/step - loss: 0.6936 - mean_positive_sim: -0.0263 - mean_negative_sim: 0.4868 - val_loss: 0.6925 - val_mean_positive_sim: -0.0247 - val_mean_negative_sim: 0.4790\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.6933 - mean_positive_sim: -0.0267 - mean_negative_sim: 0.4849 - val_loss: 0.6921 - val_mean_positive_sim: -0.0482 - val_mean_negative_sim: 0.4763\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 0.6931 - mean_positive_sim: -0.0364 - mean_negative_sim: 0.4830 - val_loss: 0.6931 - val_mean_positive_sim: -0.0133 - val_mean_negative_sim: 0.4943\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 301s 301ms/step - loss: 0.6936 - mean_positive_sim: -0.0081 - mean_negative_sim: 0.4950 - val_loss: 0.6927 - val_mean_positive_sim: -0.0330 - val_mean_negative_sim: 0.4848\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 292s 292ms/step - loss: 0.6920 - mean_positive_sim: -0.0565 - mean_negative_sim: 0.4703 - val_loss: 0.6924 - val_mean_positive_sim: -0.0642 - val_mean_negative_sim: 0.4781\n",
      "Epoch 12/50\n",
      " 628/1000 [=================>............] - ETA: 1:26 - loss: 0.6930 - mean_positive_sim: -0.0457 - mean_negative_sim: 0.4782"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node loss/lambda_1_loss/Mean_2 (defined at /usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1398) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node loss/add_4 (defined at /usr/lib/python3.7/site-packages/keras/engine/training.py:360) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'loss/lambda_1_loss/Mean_2', defined at:\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 542, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3215, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-50668ab2517d>\", line 33, in <module>\n    mean_positive_sim, mean_negative_sim,\n  File \"/usr/lib/python3.7/site-packages/keras/engine/training.py\", line 342, in compile\n    sample_weight, mask)\n  File \"/usr/lib/python3.7/site-packages/keras/engine/training_utils.py\", line 422, in weighted\n    score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n  File \"/usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 1398, in mean\n    return tf.reduce_mean(x, axis, keepdims)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1534, in reduce_mean_v1\n    return reduce_mean(input_tensor, axis, keepdims, name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1592, in reduce_mean\n    name=name))\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 5571, in mean\n    name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node loss/lambda_1_loss/Mean_2 (defined at /usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1398) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node loss/add_4 (defined at /usr/lib/python3.7/site-packages/keras/engine/training.py:360) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node loss/lambda_1_loss/Mean_2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss/add_4}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-78e65db48fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mquery_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_code_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     ),\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m                         \u001b[0;34m'Feeding from symbolic tensors is not '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m                         'supported with sparse inputs.')\n\u001b[0;32m-> 2703\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m             \u001b[0;31m# callable generated by Session._make_callable_from_options accepts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node loss/lambda_1_loss/Mean_2 (defined at /usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1398) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node loss/add_4 (defined at /usr/lib/python3.7/site-packages/keras/engine/training.py:360) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'loss/lambda_1_loss/Mean_2', defined at:\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 542, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3215, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-50668ab2517d>\", line 33, in <module>\n    mean_positive_sim, mean_negative_sim,\n  File \"/usr/lib/python3.7/site-packages/keras/engine/training.py\", line 342, in compile\n    sample_weight, mask)\n  File \"/usr/lib/python3.7/site-packages/keras/engine/training_utils.py\", line 422, in weighted\n    score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n  File \"/usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 1398, in mean\n    return tf.reduce_mean(x, axis, keepdims)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1534, in reduce_mean_v1\n    return reduce_mean(input_tensor, axis, keepdims, name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1592, in reduce_mean\n    name=name))\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 5571, in mean\n    name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node loss/lambda_1_loss/Mean_2 (defined at /usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1398) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node loss/add_4 (defined at /usr/lib/python3.7/site-packages/keras/engine/training.py:360) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "training_history = model.fit_generator(\n",
    "    generator=sparse_docs_batch_generator(\n",
    "        queries_train, docs_train, encoded_docs,\n",
    "        query_encoder, batch_size, doc_code_size,\n",
    "    ),\n",
    "    epochs=50,\n",
    "    steps_per_epoch=1000,\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(batch_size=batch_size),\n",
    "        keras.callbacks.ModelCheckpoint('doc_bow_dssm.bin', monitor='val_loss', save_best_only=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5, min_delta=0.0002),\n",
    "    ],\n",
    "    validation_data=sparse_docs_batch_generator(\n",
    "        queries_val, docs_val, encoded_docs,\n",
    "        query_encoder, batch_size, doc_code_size,\n",
    "    ),\n",
    "    validation_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_encoder.encode('The silence of the Somme: Sound and realism in British and Dutch poems mediating The Battle of the Somme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_encoder.encode_with_padding([\n",
    "        'The silence of the Somme: Sound and realism in British '\n",
    "        'and Dutch poems mediating The Battle of the Somme',\n",
    "        'runescape'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(sparse_docs_batch_generator(\n",
    "        queries_train, docs_train, encoded_docs,\n",
    "        query_encoder, batch_size, doc_code_size,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\n",
    "    get_sparse_docs(encoded_docs, doc_code_size, [\n",
    "        '/wiki/Battle_of_the_Somme',\n",
    "        '/wiki/RuneScape',\n",
    "        '/wiki/Liliaceae',\n",
    "        '/wiki/Annual_percentage_rate'\n",
    "    ]),\n",
    "    query_encoder.encode_with_padding([\n",
    "        'the silence of the somme: sound and realism in british '\n",
    "        'and dutch poems mediating the battle of the somme',\n",
    "        'runescape',\n",
    "        'liliaceae - tulipa lord beaconsfield + parmesiano',\n",
    "        'one-tenth of the amount to payday max apr if you borrow one hundred '\n",
    "        'dollars in a fourteen-day period the highest yearly percentage rate '\n",
    "        'could be 309 max amount there isn t any specified amount of',\n",
    "    ]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_query(encoded_docs, doc_code_size, query, query_encoder, batch_size):\n",
    "    all_docs = list(encoded_docs.keys())\n",
    "    query_code = np.tile(np.reshape(query_encoder.encode(query), (1, -1)), (batch_size, 1))\n",
    "    predictions = np.array([])\n",
    "    for batch_ind in tqdm.tqdm(range(int(ceil(len(all_docs) / batch_size)))):\n",
    "        prediction = model.predict_generator([\n",
    "            get_sparse_docs(\n",
    "                encoded_docs, doc_code_size,\n",
    "                all_docs[batch_ind * batch_size: batch_ind * (batch_size + 1)]\n",
    "            ),\n",
    "            query_code\n",
    "        ])\n",
    "        print(prediction)\n",
    "        predictions = np.concatenate([predictions, np.reshape(prediction, (batch_size,))])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2061 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (32,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a981d6763ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_for_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_code_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'enigma machine who inventor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-b2ee567df369>\u001b[0m in \u001b[0;36mpredict_for_query\u001b[0;34m(encoded_docs, doc_code_size, query, query_encoder, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m         ])\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    290\u001b[0m            [5, 6]])\n\u001b[1;32m    291\u001b[0m     \"\"\"\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (32,)"
     ]
    }
   ],
   "source": [
    "predict_for_query(encoded_docs, doc_code_size, 'enigma machine who inventor', query_encoder, 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
