{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_storage import FileStorage\n",
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import transliterate\n",
    "import tqdm\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLITERATE_DICT = {\n",
    "    'а': 'a',\n",
    "    'б': 'b',\n",
    "    'в': 'v',\n",
    "    'г': 'g',\n",
    "    'д': 'd',\n",
    "    'е': 'e',\n",
    "    'ж': 'zh',\n",
    "    'з': 'z',\n",
    "    'и': 'i',\n",
    "    'к': 'k',\n",
    "    'л': 'l',\n",
    "    'м': 'm',\n",
    "    'н': 'n',\n",
    "    'о': 'o',\n",
    "    'п': 'p',\n",
    "    'р': 'r',\n",
    "    'с': 's',\n",
    "    'т': 't',\n",
    "    'ф': 'f',\n",
    "    'х': 'h',\n",
    "    'ц': 'ts',\n",
    "    'ч': 'ch',\n",
    "    'ш': 'sh',\n",
    "    'щ': 'sch',\n",
    "    'ъ': \"'\",\n",
    "    'ы': 'y',\n",
    "    'ь': \"'\",\n",
    "    'э': 'e',\n",
    "    'ю': 'ju',\n",
    "    'я': 'ya',\n",
    "    'π': 'pi',\n",
    "    'ı': 'i',\n",
    "    'ə': 'e',\n",
    "    'ل': 'j',\n",
    "    'ƒ': 'f',\n",
    "    'ﬁ': 'fi',\n",
    "    '\\xad': '-',\n",
    "    'µ': 'mu',\n",
    "    '\\u200b': ' ',\n",
    "    'ː': ':',\n",
    "    '—': '-',\n",
    "    '−': '-',\n",
    "    '–': '-',\n",
    "    '”': '\"',\n",
    "    '“': '\"',\n",
    "    '«': '\"',\n",
    "    '»': '\"',\n",
    "    'у': 'y',\n",
    "    '’': '\"',\n",
    "    '‘': '\"',\n",
    "    '`': \"'\",\n",
    "    '„': '\"',\n",
    "    '·': ',',\n",
    "    '•': ',',\n",
    "    '…': ' ',\n",
    "    # https://www.redhat.com/archives/fedora-extras-commits/2007-June/msg03617.html\n",
    "    \"\\u0621\": \"'\", # hamza-on-the-line\n",
    "    \"\\u0622\": \"|\", # madda\n",
    "    \"\\u0623\": \">\", # hamza-on-'alif\n",
    "    \"\\u0624\": \"&\", # hamza-on-waaw\n",
    "    \"\\u0625\": \"<\", # hamza-under-'alif\n",
    "    \"\\u0626\": \"}\", # hamza-on-yaa'\n",
    "    \"\\u0627\": \"A\", # bare 'alif\n",
    "    \"\\u0628\": \"b\", # baa'\n",
    "    \"\\u0629\": \"p\", # taa' marbuuTa\n",
    "    \"\\u062A\": \"t\", # taa'\n",
    "    \"\\u062B\": \"v\", # thaa'\n",
    "    \"\\u062C\": \"j\", # jiim\n",
    "    \"\\u062D\": \"H\", # Haa'\n",
    "    \"\\u062E\": \"x\", # khaa'\n",
    "    \"\\u062F\": \"d\", # daal\n",
    "    \"\\u0630\": \"*\", # dhaal\n",
    "    \"\\u0631\": \"r\", # raa'\n",
    "    \"\\u0632\": \"z\", # zaay\n",
    "    \"\\u0633\": \"s\", # siin\n",
    "    \"\\u0634\": \"$\", # shiin\n",
    "    \"\\u0635\": \"S\", # Saad\n",
    "    \"\\u0636\": \"D\", # Daad\n",
    "    \"\\u0637\": \"T\", # Taa'\n",
    "    \"\\u0638\": \"Z\", # Zaa' (DHaa')\n",
    "    \"\\u0639\": \"E\", # cayn\n",
    "    \"\\u063A\": \"g\", # ghayn\n",
    "    \"\\u0640\": \"_\", # taTwiil\n",
    "    \"\\u0641\": \"f\", # faa'\n",
    "    \"\\u0642\": \"q\", # qaaf\n",
    "    \"\\u0643\": \"k\", # kaaf\n",
    "    \"\\u0644\": \"l\", # laam\n",
    "    \"\\u0645\": \"m\", # miim\n",
    "    \"\\u0646\": \"n\", # nuun\n",
    "    \"\\u0647\": \"h\", # haa'\n",
    "    \"\\u0648\": \"w\", # waaw\n",
    "    \"\\u0649\": \"Y\", # 'alif maqSuura\n",
    "    \"\\u064A\": \"y\", # yaa'\n",
    "    \"\\u064B\": \"F\", # fatHatayn\n",
    "    \"\\u064C\": \"N\", # Dammatayn\n",
    "    \"\\u064D\": \"K\", # kasratayn\n",
    "    \"\\u064E\": \"a\", # fatHa\n",
    "    \"\\u064F\": \"u\", # Damma\n",
    "    \"\\u0650\": \"i\", # kasra\n",
    "    \"\\u0651\": \"~\", # shaddah\n",
    "    \"\\u0652\": \"o\", # sukuun\n",
    "    \"\\u0670\": \"`\", # dagger 'alif\n",
    "    \"\\u0671\": \"{\", # waSlaﬁ\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34753821/remove-diacritics-from-string-for-search-function\n",
    "\n",
    "def shave_marks(txt):\n",
    "    \"\"\"This method removes all diacritic marks from the given string\"\"\"\n",
    "    norm_txt = unicodedata.normalize('NFD', txt)\n",
    "    shaved = ''.join(c for c in norm_txt if not unicodedata.combining(c))\n",
    "    return unicodedata.normalize('NFC', shaved)\n",
    "\n",
    "\n",
    "def is_english_letters(string):\n",
    "    return re.search(r'[^a-zA-Z0-9°_©®™;§,№!#@.×:+=()/£¥€$|<>~{}\\\\\\[\\]%&*^?\"\\'-]', string) is None\n",
    "\n",
    "\n",
    "def try_transliterate(query):\n",
    "    query = unicodedata.normalize('NFC', shave_marks(query).lower())\n",
    "    try:\n",
    "        return transliterate.translit(query, reversed=True)\n",
    "    except transliterate.exceptions.LanguageDetectionError as query_error:\n",
    "        transliteration = []\n",
    "        for word in query.split():\n",
    "            if is_english_letters(word):\n",
    "                transliteration.append(word)\n",
    "            else:\n",
    "                try:\n",
    "                    transliteration.append(transliterate.translit(word, reversed=True))\n",
    "                except transliterate.exceptions.LanguageDetectionError as e:\n",
    "                    new_word = []\n",
    "                    for ch in word:\n",
    "                        translited_ch = TRANSLITERATE_DICT.get(ch, ch)\n",
    "                        new_word.append(translited_ch)\n",
    "                    transliteration.append(''.join(new_word))\n",
    "        return ' '.join(transliteration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500000it [00:55, 9059.98it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('req_ans_learn.tsv', encoding='utf-8-sig') as train_file, open('transliterated_learn.tsv', 'w') as transliterated_learn_file:\n",
    "    for line in tqdm.tqdm(train_file):\n",
    "        query, url_end = line.strip().split('\\t')\n",
    "        transliteration = try_transliterate(query)\n",
    "        transliterated_learn_file.write(transliteration + '\\t' + url_end + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_storage = FileStorage('../filtered_storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151120"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.quora.com/How-can-I-extract-only-text-data-from-HTML-pages\n",
    "\n",
    "def get_text(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    data = soup.findAll(text=True)\n",
    "\n",
    "    def informative(element):\n",
    "        if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "            return False\n",
    "        elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "            return False\n",
    "        elif len(element) < 20:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    return [line.split() for line in data if informative(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGINNING = 'https://simple.wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text(file_storage.read(BEGINNING + '/wiki/Germany'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(string):\n",
    "    return [word for word in string.lower().split(',.!?;: \\t\\n') if word]\n",
    "\n",
    "\n",
    "def get_nrgams(n, word_list):\n",
    "    if n == 1:\n",
    "        return sum(word_list)\n",
    "    else:\n",
    "        return [\n",
    "            word[i:i + n] for i in range(len(word) - n + 1)\n",
    "            for word in word_list\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_n_gram_counter(n, collection):\n",
    "    counter = Counter()\n",
    "    for element in collection:\n",
    "        counter_update(n, get_words(element))\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    def __init__(self, elements):\n",
    "        self._ind_to_elem = elements\n",
    "        self._elem_to_ind = {elem: ind for ind, elem in enumerate(elements)}\n",
    "        \n",
    "    def get_elem(self, ind):\n",
    "        return self._ind_to_elem[ind]\n",
    "    \n",
    "    def get_ind(self, elem, default=None):\n",
    "        return self._elem_to_ind.get(elem, default)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._ind_to_elem)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_counter(cls, counter, n_most_common):\n",
    "        return Storage([elem for elem, _ in counter.most_common(n_most_common)])\n",
    "    \n",
    "\n",
    "class Encoder:\n",
    "    def encode(self, example):\n",
    "        raise NotImplemented()\n",
    "\n",
    "    def encode_with_padding(self, examples):\n",
    "        codes = [self.encode(example) for example in examples]\n",
    "        max_len = max(map(len, codes))\n",
    "        return np.array([\n",
    "            np.concatenate([code, np.zeros(len(code) - max_len)])\n",
    "        ], dtype=np.int32)\n",
    "\n",
    "\n",
    "class BagOfNgramsEncoder(Encoder):\n",
    "    def __init__(self, collection, ngram_number_array, max_size):\n",
    "        self._ngram_storages = [\n",
    "            Storage.from_counter(get_n_gram_counter(n + 1, collection), ngram_nubmer)\n",
    "            for n, ngram_number in enumerate(ngram_number_array)\n",
    "        ]\n",
    "        self._code_size = sum(map(len, self._ngram_storages)) + 2\n",
    "        self._max_size = max_size - self.n + 1\n",
    "        \n",
    "    @property\n",
    "    def max_size(self):\n",
    "        return self._max_size\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return len(self._ngram_storages)\n",
    "    \n",
    "    def _encode_word(self, word):\n",
    "        for i in range(len(word) - self.n + 1)\n",
    "            ind = 1\n",
    "            for ngram_len in reversed(range(1, self.n + 1)):\n",
    "                ngram = word[i:i + ngram_len]\n",
    "                ngram_ind = self._ngram_storages[ngram_len - 1].get_ind(ngram)\n",
    "                if ngram_ind is None:\n",
    "                    ind += len(self._ngram_storages[ngram_len - 1])\n",
    "                else:\n",
    "                    ind += ngram_ind\n",
    "                    return ind\n",
    "            return ind\n",
    "\n",
    "    def encode(self, string):\n",
    "        return sum(self._encode_word(word) for word in get_words(string))\n",
    "        \n",
    "    @property\n",
    "    def code_size(self):\n",
    "        return self._code_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocEncoder(Encoder):    \n",
    "    def __init__(self, collection, n_words, n_trigrams):\n",
    "        trigram_counter = Counter()\n",
    "        word_counter = Counter()\n",
    "        \n",
    "        for example in collection:\n",
    "            words, trigrams = self._prepare_example(example)\n",
    "            word_counter.update(words)\n",
    "            trigram_counter.udpate(trigrams)\n",
    "\n",
    "        self._word_storage = Storage.from_counter(word_counter, n_words)\n",
    "        self._trigram_storage = Storage.from_counter(trigram_counter, n_trigrams)    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _prepare_example(example):\n",
    "        words = []\n",
    "        trigrams = []\n",
    "        for example_part in example:\n",
    "            curr_words = get_words(example_part)\n",
    "            words += curr_words\n",
    "            trigrams += get_nrgams(3, curr_words)\n",
    "        return words, trigrams\n",
    "    \n",
    "    def _encode_word(self, word):\n",
    "        word_code = self._word_storage.get_ind(word)\n",
    "        if word_code is None:\n",
    "            return [\n",
    "                self._trigram_storage.get_ind(trigram, self._unk_ind)\n",
    "                for trigram in get_nrgams(3, [word])\n",
    "            ]\n",
    "        else:\n",
    "            return [word_code]\n",
    "    \n",
    "    def encode(self, example):\n",
    "        return sum(\n",
    "            self._encode_word(word)\n",
    "            for word in get_words(example_part)\n",
    "            for example_part in example\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def _unk_ind(self):\n",
    "        return self.code_size - 1\n",
    "                   \n",
    "    @property\n",
    "    def _pad_ind(self):\n",
    "        return 0\n",
    "                                                  \n",
    "    @property\n",
    "    def code_size(self):\n",
    "        return len(self._word_storage) + len(self._trigram_storage) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchers_generator(queries, doc_names, doc_storage, doc_encoder, query_encoder, batch_size):\n",
    "    np.choi len(queries)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense(units, activation='relu'):\n",
    "    return keras.layers.Dense(\n",
    "        units, activation=activation, kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = keras.layers.Input(shape=(None,), dtype='int32')\n",
    "query = keras.layers.Input(shape=(None,), dtype='int32')\n",
    "\n",
    "def get_embed(embed_layers, data):\n",
    "    for layer in embed_layers:\n",
    "        data = layer(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "doc_embed_layers = [\n",
    "    keras.layers.Embedding(encoder.code_size, 256),\n",
    "    keras.layers.LSTM(256),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(256),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "query_embed_layers = [\n",
    "    keras.layers.Embedding(encoder.code_size, 128),\n",
    "    keras.layers.LSTM(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "doc_embed = get_embed(doc_embed_layers, doc)\n",
    "query_embed = get_embed(query_embed_layers, query)\n",
    "\n",
    "similarity = keras.layers.Dot(axes=1, normalize=True)([doc_embed, query_embed])\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[doc, query],\n",
    "    outputs=similarity,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
