{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_storage import FileStorage\n",
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import transliterate\n",
    "import tqdm\n",
    "import unicodedata\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLITERATE_DICT = {\n",
    "    'а': 'a',\n",
    "    'б': 'b',\n",
    "    'в': 'v',\n",
    "    'г': 'g',\n",
    "    'д': 'd',\n",
    "    'е': 'e',\n",
    "    'ж': 'zh',\n",
    "    'з': 'z',\n",
    "    'и': 'i',\n",
    "    'к': 'k',\n",
    "    'л': 'l',\n",
    "    'м': 'm',\n",
    "    'н': 'n',\n",
    "    'о': 'o',\n",
    "    'п': 'p',\n",
    "    'р': 'r',\n",
    "    'с': 's',\n",
    "    'т': 't',\n",
    "    'ф': 'f',\n",
    "    'х': 'h',\n",
    "    'ц': 'ts',\n",
    "    'ч': 'ch',\n",
    "    'ш': 'sh',\n",
    "    'щ': 'sch',\n",
    "    'ъ': \"'\",\n",
    "    'ы': 'y',\n",
    "    'ь': \"'\",\n",
    "    'э': 'e',\n",
    "    'ю': 'ju',\n",
    "    'я': 'ya',\n",
    "    'π': 'pi',\n",
    "    'ı': 'i',\n",
    "    'ə': 'e',\n",
    "    'ل': 'j',\n",
    "    'ƒ': 'f',\n",
    "    'ﬁ': 'fi',\n",
    "    '\\xad': '-',\n",
    "    'µ': 'mu',\n",
    "    '\\u200b': ' ',\n",
    "    'ː': ':',\n",
    "    '—': '-',\n",
    "    '−': '-',\n",
    "    '–': '-',\n",
    "    '”': '\"',\n",
    "    '“': '\"',\n",
    "    '«': '\"',\n",
    "    '»': '\"',\n",
    "    'у': 'y',\n",
    "    '’': '\"',\n",
    "    '‘': '\"',\n",
    "    '`': \"'\",\n",
    "    '„': '\"',\n",
    "    '·': ',',\n",
    "    '•': ',',\n",
    "    '…': ' ',\n",
    "    # https://www.redhat.com/archives/fedora-extras-commits/2007-June/msg03617.html\n",
    "    \"\\u0621\": \"'\", # hamza-on-the-line\n",
    "    \"\\u0622\": \"|\", # madda\n",
    "    \"\\u0623\": \">\", # hamza-on-'alif\n",
    "    \"\\u0624\": \"&\", # hamza-on-waaw\n",
    "    \"\\u0625\": \"<\", # hamza-under-'alif\n",
    "    \"\\u0626\": \"}\", # hamza-on-yaa'\n",
    "    \"\\u0627\": \"A\", # bare 'alif\n",
    "    \"\\u0628\": \"b\", # baa'\n",
    "    \"\\u0629\": \"p\", # taa' marbuuTa\n",
    "    \"\\u062A\": \"t\", # taa'\n",
    "    \"\\u062B\": \"v\", # thaa'\n",
    "    \"\\u062C\": \"j\", # jiim\n",
    "    \"\\u062D\": \"H\", # Haa'\n",
    "    \"\\u062E\": \"x\", # khaa'\n",
    "    \"\\u062F\": \"d\", # daal\n",
    "    \"\\u0630\": \"*\", # dhaal\n",
    "    \"\\u0631\": \"r\", # raa'\n",
    "    \"\\u0632\": \"z\", # zaay\n",
    "    \"\\u0633\": \"s\", # siin\n",
    "    \"\\u0634\": \"$\", # shiin\n",
    "    \"\\u0635\": \"S\", # Saad\n",
    "    \"\\u0636\": \"D\", # Daad\n",
    "    \"\\u0637\": \"T\", # Taa'\n",
    "    \"\\u0638\": \"Z\", # Zaa' (DHaa')\n",
    "    \"\\u0639\": \"E\", # cayn\n",
    "    \"\\u063A\": \"g\", # ghayn\n",
    "    \"\\u0640\": \"_\", # taTwiil\n",
    "    \"\\u0641\": \"f\", # faa'\n",
    "    \"\\u0642\": \"q\", # qaaf\n",
    "    \"\\u0643\": \"k\", # kaaf\n",
    "    \"\\u0644\": \"l\", # laam\n",
    "    \"\\u0645\": \"m\", # miim\n",
    "    \"\\u0646\": \"n\", # nuun\n",
    "    \"\\u0647\": \"h\", # haa'\n",
    "    \"\\u0648\": \"w\", # waaw\n",
    "    \"\\u0649\": \"Y\", # 'alif maqSuura\n",
    "    \"\\u064A\": \"y\", # yaa'\n",
    "    \"\\u064B\": \"F\", # fatHatayn\n",
    "    \"\\u064C\": \"N\", # Dammatayn\n",
    "    \"\\u064D\": \"K\", # kasratayn\n",
    "    \"\\u064E\": \"a\", # fatHa\n",
    "    \"\\u064F\": \"u\", # Damma\n",
    "    \"\\u0650\": \"i\", # kasra\n",
    "    \"\\u0651\": \"~\", # shaddah\n",
    "    \"\\u0652\": \"o\", # sukuun\n",
    "    \"\\u0670\": \"`\", # dagger 'alif\n",
    "    \"\\u0671\": \"{\", # waSlaﬁ\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34753821/remove-diacritics-from-string-for-search-function\n",
    "\n",
    "def shave_marks(txt):\n",
    "    \"\"\"This method removes all diacritic marks from the given string\"\"\"\n",
    "    norm_txt = unicodedata.normalize('NFD', txt)\n",
    "    shaved = ''.join(c for c in norm_txt if not unicodedata.combining(c))\n",
    "    return unicodedata.normalize('NFC', shaved)\n",
    "\n",
    "\n",
    "def is_english_letters(string):\n",
    "    return re.search(r'[^a-zA-Z0-9°_©®™;§,№!#@.×:+=()/£¥€$|<>~{}\\\\\\[\\]%&*^?\"\\'-]', string) is None\n",
    "\n",
    "\n",
    "def try_transliterate(query):\n",
    "    query = unicodedata.normalize('NFC', shave_marks(query).lower())\n",
    "    try:\n",
    "        return transliterate.translit(query, reversed=True)\n",
    "    except transliterate.exceptions.LanguageDetectionError as query_error:\n",
    "        transliteration = []\n",
    "        for word in query.split():\n",
    "            if is_english_letters(word):\n",
    "                transliteration.append(word)\n",
    "            else:\n",
    "                try:\n",
    "                    transliteration.append(transliterate.translit(word, reversed=True))\n",
    "                except transliterate.exceptions.LanguageDetectionError as e:\n",
    "                    new_word = []\n",
    "                    for ch in word:\n",
    "                        translited_ch = TRANSLITERATE_DICT.get(ch, ch)\n",
    "                        new_word.append(translited_ch)\n",
    "                    transliteration.append(''.join(new_word))\n",
    "        return ' '.join(transliteration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500000it [00:55, 9059.98it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('req_ans_learn.tsv', encoding='utf-8-sig') as train_file, open('transliterated_learn.tsv', 'w') as transliterated_learn_file:\n",
    "    for line in tqdm.tqdm(train_file):\n",
    "        query, url_end = line.strip().split('\\t')\n",
    "        transliteration = try_transliterate(query)\n",
    "        transliterated_learn_file.write(transliteration + '\\t' + url_end + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_storage = FileStorage('../filtered_storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151120"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.quora.com/How-can-I-extract-only-text-data-from-HTML-pages\n",
    "\n",
    "def get_text(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    data = soup.findAll(text=True)\n",
    "\n",
    "    def informative(element):\n",
    "        if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "            return False\n",
    "        elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "            return False\n",
    "        elif len(element) < 20:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    return [line.split() for line in data if informative(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGINNING = 'https://simple.wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text(file_storage.read(BEGINNING + '/wiki/Germany'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_str(string):\n",
    "    return string.lower()\n",
    "\n",
    "\n",
    "def get_n_gram_counter(n, collection):\n",
    "    counter = Counter()\n",
    "    for element in collection:\n",
    "        element = normalize_str(element)\n",
    "        if n == 1:\n",
    "            counter.update(element)\n",
    "        else:\n",
    "            counter.update([\n",
    "                element[ind:ind + n]\n",
    "                for ind in range(len(element) - n)\n",
    "            ])\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    def __init__(self, elements):\n",
    "        self._ind_to_elem = elements\n",
    "        self._elem_to_ind = {elem: ind for ind, elem in enumerate(elements)}\n",
    "        \n",
    "    def get_elem(self, ind):\n",
    "        return self._ind_to_elem[ind]\n",
    "    \n",
    "    def get_ind(self, elem):\n",
    "        return self._elem_to_ind.get(elem)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._ind_to_elem)\n",
    "\n",
    "\n",
    "class BagOfNgramsEncoder:\n",
    "    def __init__(self, ngrams_array, max_size):\n",
    "        self._ngram_storages = [Storage(ngrams) for ngrams in ngrams_array]\n",
    "        self._code_size = sum(map(len, self._ngram_storages)) + 2\n",
    "        self._max_size = max_size - self.n + 1\n",
    "        \n",
    "    @property\n",
    "    def max_size(self):\n",
    "        return self._max_size\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return len(self._ngram_storages)\n",
    "\n",
    "    def encode(self, string, max_size=None):\n",
    "        if max_size is None:\n",
    "            max_size = self.max_size\n",
    "        else:\n",
    "            max_size = max_size - self.n + 1\n",
    "        string = normalize_str(string)\n",
    "        code = []\n",
    "        \n",
    "        real_len = min(len(string) - self.n + 1, max_size)\n",
    "        for i in range(real_len):\n",
    "            ind = 0\n",
    "            for ngram_len in reversed(range(1, self.n + 1)):\n",
    "                ngram = string[i:i + ngram_len]\n",
    "                ngram_ind = self._ngram_storages[ngram_len - 1].get_ind(ngram)\n",
    "                if ngram_ind is None:\n",
    "                    ind += len(self._ngram_storages[ngram_len - 1])\n",
    "                else:\n",
    "                    ind += ngram_ind\n",
    "                    break\n",
    "            code.append(ind)\n",
    "        code += [self.code_size - 1] * (max_size - real_len)\n",
    "        return np.array(code)\n",
    "        \n",
    "    @property\n",
    "    def code_size(self):\n",
    "        return self._code_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_array(encoder, array, max_size=None):\n",
    "    max_size = max_size or max(map(len, array))\n",
    "    return np.array([encoder.encode(x, max_size) for x in array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_ngrams = []\n",
    "for n, most_common in zip(range(1, 4), [300, 2200, 7500]):\n",
    "    counter = get_n_gram_counter(n, X_train)\n",
    "    most_common = counter.most_common(most_common)\n",
    "    most_common_ngrams.append([ngram for ngram, _ in most_common])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense(units, activation='relu'):\n",
    "    return keras.layers.Dense(\n",
    "        units, activation=activation, kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = keras.layers.Input(shape=(None,), dtype='int32')\n",
    "query = keras.layers.Input(shape=(None,), dtype='int32')\n",
    "\n",
    "def get_embed(embed_layers, data):\n",
    "    for layer in embed_layers:\n",
    "        data = layer(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "doc_embed_layers = [\n",
    "    keras.layers.Embedding(encoder.code_size, 256),\n",
    "    keras.layers.LSTM(256),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(256),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "query_embed_layers = [\n",
    "    keras.layers.Embedding(encoder.code_size, 128),\n",
    "    keras.layers.LSTM(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "doc_embed = get_embed(doc_embed_layers, doc)\n",
    "query_embed = get_embed(query_embed_layers, query)\n",
    "\n",
    "similarity = keras.layers.Dot(axes=1, normalize=True)([doc_embed, query_embed])\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[doc, query],\n",
    "    outputs=similarity,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
