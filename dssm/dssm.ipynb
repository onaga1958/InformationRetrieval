{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from file_storage import FileStorage\n",
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import transliterate\n",
    "import tqdm\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_storage = FileStorage('../filtered_storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157154"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем транслитерацию. Есть библиотека transliterate, используя её будем транслитерировать в несколько стадий:\n",
    "1. Приводим к нижнему регистру и убираем диакритику\n",
    "1. Транслитерируем весь запрос библиотекой\n",
    "2. Транслитерируем пословно библиотекой\n",
    "3. Транслитерируем с помощью словаря ниже\n",
    "4. Оставшееся оставляем как есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLITERATE_DICT = {\n",
    "    'а': 'a',\n",
    "    'б': 'b',\n",
    "    'в': 'v',\n",
    "    'г': 'g',\n",
    "    'д': 'd',\n",
    "    'е': 'e',\n",
    "    'ж': 'zh',\n",
    "    'з': 'z',\n",
    "    'и': 'i',\n",
    "    'к': 'k',\n",
    "    'л': 'l',\n",
    "    'м': 'm',\n",
    "    'н': 'n',\n",
    "    'о': 'o',\n",
    "    'п': 'p',\n",
    "    'р': 'r',\n",
    "    'с': 's',\n",
    "    'т': 't',\n",
    "    'ф': 'f',\n",
    "    'х': 'h',\n",
    "    'ц': 'ts',\n",
    "    'ч': 'ch',\n",
    "    'ш': 'sh',\n",
    "    'щ': 'sch',\n",
    "    'ъ': \"'\",\n",
    "    'ы': 'y',\n",
    "    'ь': \"'\",\n",
    "    'э': 'e',\n",
    "    'ю': 'ju',\n",
    "    'я': 'ya',\n",
    "    'π': 'pi',\n",
    "    'ı': 'i',\n",
    "    'ə': 'e',\n",
    "    'ل': 'j',\n",
    "    'ƒ': 'f',\n",
    "    'ﬁ': 'fi',\n",
    "    '\\xad': '-',\n",
    "    'µ': 'mu',\n",
    "    '\\u200b': ' ',\n",
    "    'ː': ':',\n",
    "    '—': '-',\n",
    "    '−': '-',\n",
    "    '–': '-',\n",
    "    '”': '\"',\n",
    "    '“': '\"',\n",
    "    '«': '\"',\n",
    "    '»': '\"',\n",
    "    'у': 'y',\n",
    "    '’': '\"',\n",
    "    '‘': '\"',\n",
    "    '`': \"'\",\n",
    "    '„': '\"',\n",
    "    '·': ',',\n",
    "    '•': ',',\n",
    "    '…': ' ',\n",
    "    # https://www.redhat.com/archives/fedora-extras-commits/2007-June/msg03617.html\n",
    "    \"\\u0621\": \"'\", # hamza-on-the-line\n",
    "    \"\\u0622\": \"|\", # madda\n",
    "    \"\\u0623\": \">\", # hamza-on-'alif\n",
    "    \"\\u0624\": \"&\", # hamza-on-waaw\n",
    "    \"\\u0625\": \"<\", # hamza-under-'alif\n",
    "    \"\\u0626\": \"}\", # hamza-on-yaa'\n",
    "    \"\\u0627\": \"A\", # bare 'alif\n",
    "    \"\\u0628\": \"b\", # baa'\n",
    "    \"\\u0629\": \"p\", # taa' marbuuTa\n",
    "    \"\\u062A\": \"t\", # taa'\n",
    "    \"\\u062B\": \"v\", # thaa'\n",
    "    \"\\u062C\": \"j\", # jiim\n",
    "    \"\\u062D\": \"H\", # Haa'\n",
    "    \"\\u062E\": \"x\", # khaa'\n",
    "    \"\\u062F\": \"d\", # daal\n",
    "    \"\\u0630\": \"*\", # dhaal\n",
    "    \"\\u0631\": \"r\", # raa'\n",
    "    \"\\u0632\": \"z\", # zaay\n",
    "    \"\\u0633\": \"s\", # siin\n",
    "    \"\\u0634\": \"$\", # shiin\n",
    "    \"\\u0635\": \"S\", # Saad\n",
    "    \"\\u0636\": \"D\", # Daad\n",
    "    \"\\u0637\": \"T\", # Taa'\n",
    "    \"\\u0638\": \"Z\", # Zaa' (DHaa')\n",
    "    \"\\u0639\": \"E\", # cayn\n",
    "    \"\\u063A\": \"g\", # ghayn\n",
    "    \"\\u0640\": \"_\", # taTwiil\n",
    "    \"\\u0641\": \"f\", # faa'\n",
    "    \"\\u0642\": \"q\", # qaaf\n",
    "    \"\\u0643\": \"k\", # kaaf\n",
    "    \"\\u0644\": \"l\", # laam\n",
    "    \"\\u0645\": \"m\", # miim\n",
    "    \"\\u0646\": \"n\", # nuun\n",
    "    \"\\u0647\": \"h\", # haa'\n",
    "    \"\\u0648\": \"w\", # waaw\n",
    "    \"\\u0649\": \"Y\", # 'alif maqSuura\n",
    "    \"\\u064A\": \"y\", # yaa'\n",
    "    \"\\u064B\": \"F\", # fatHatayn\n",
    "    \"\\u064C\": \"N\", # Dammatayn\n",
    "    \"\\u064D\": \"K\", # kasratayn\n",
    "    \"\\u064E\": \"a\", # fatHa\n",
    "    \"\\u064F\": \"u\", # Damma\n",
    "    \"\\u0650\": \"i\", # kasra\n",
    "    \"\\u0651\": \"~\", # shaddah\n",
    "    \"\\u0652\": \"o\", # sukuun\n",
    "    \"\\u0670\": \"`\", # dagger 'alif\n",
    "    \"\\u0671\": \"{\", # waSlaﬁ\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_from_the_internet(url):\n",
    "    try:\n",
    "        return urlopen(url).read().decode('utf-8')\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except urllib.error.HTTPError as e:\n",
    "        code = e.code\n",
    "        if code != 404:\n",
    "            print(e)\n",
    "        return code\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34753821/remove-diacritics-from-string-for-search-function\n",
    "\n",
    "def shave_marks(txt):\n",
    "    \"\"\"This method removes all diacritic marks from the given string\"\"\"\n",
    "    norm_txt = unicodedata.normalize('NFD', txt)\n",
    "    shaved = ''.join(c for c in norm_txt if not unicodedata.combining(c))\n",
    "    return unicodedata.normalize('NFC', shaved)\n",
    "\n",
    "\n",
    "def is_english_letters(string):\n",
    "    return re.search(r'[^a-zA-Z0-9°_©®™;§,№!#@.×:+=()/£¥€$|<>~{}\\\\\\[\\]%&*^?\"\\'-]', string) is None\n",
    "\n",
    "\n",
    "def try_transliterate(query):\n",
    "    query = unicodedata.normalize('NFC', shave_marks(query).lower())\n",
    "    try:\n",
    "        return transliterate.translit(query, reversed=True)\n",
    "    except transliterate.exceptions.LanguageDetectionError as query_error:\n",
    "        transliteration = []\n",
    "        for word in query.split():\n",
    "            if is_english_letters(word):\n",
    "                transliteration.append(word)\n",
    "            else:\n",
    "                try:\n",
    "                    transliteration.append(transliterate.translit(word, reversed=True))\n",
    "                except transliterate.exceptions.LanguageDetectionError as e:\n",
    "                    new_word = []\n",
    "                    for ch in word:\n",
    "                        translited_ch = TRANSLITERATE_DICT.get(ch, ch)\n",
    "                        new_word.append(translited_ch)\n",
    "                    transliteration.append(''.join(new_word))\n",
    "        return ' '.join(transliteration) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяем транслитерацию к запросам из обучающей выборки и сохраняем что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50786it [00:06, 9098.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b03dc7fde968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtransliteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_transliterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtransliterated_learn_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransliteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl_end\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-c523da0d854d>\u001b[0m in \u001b[0;36mtry_transliterate\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NFC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshave_marks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransliterate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtransliterate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLanguageDetectionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mquery_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtransliteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transliterate/utils.py\u001b[0m in \u001b[0;36mtranslit\u001b[0;34m(value, language_code, reversed, strict)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlanguage_code\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mlanguage_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfail_silently\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transliterate/utils.py\u001b[0m in \u001b[0;36mdetect_language\u001b[0;34m(text, num_words, fail_silently, heavy_check)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mletter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlanguage_pack\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable_language_packs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mlanguage_pack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlanguage_pack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                     \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage_pack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_code\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transliterate/base.py\u001b[0m in \u001b[0;36mcontains\u001b[0;34m(cls, character)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mchar_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NFC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mchar_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mcharacter_range\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharacter_ranges\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m                 \u001b[0mrange_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcharacter_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0mrange_upper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcharacter_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('req_ans_learn.tsv', encoding='utf-8-sig') as train_file, open('transliterated_learn.tsv', 'w') as transliterated_learn_file:\n",
    "    for line in tqdm.tqdm(train_file):\n",
    "        query, url_end = line.strip().split('\\t')\n",
    "        transliteration = try_transliterate(query)\n",
    "        transliterated_learn_file.write(transliteration + '\\t' + url_end + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Достанем текст из запроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.quora.com/How-can-I-extract-only-text-data-from-HTML-pages\n",
    "\n",
    "def informative(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    elif len(element) < 10:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_text(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    data = soup.findAll(text=True)\n",
    "\n",
    "    text = []\n",
    "    informative_lines = [line for line in data if informative(line)]\n",
    "    started = False\n",
    "    for line in data:\n",
    "        if line.strip() == 'Jump to search':\n",
    "            started = True\n",
    "            continue\n",
    "        if started and informative(line):\n",
    "            if re.search(r'Cached time: \\d+\\nCache expiry: \\d+', line) is not None:\n",
    "                break\n",
    "            text.append(line.strip())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGINNING = 'https://simple.wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text(file_storage.read(BEGINNING + '/wiki/Germany'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем энкодинг запросов и документов.\n",
    "Запросы будет кодирвать по ngram'ам, а документы по словам и триграммам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(string):\n",
    "    return [word for word in re.split('[,.!?;:() \\t\\n]', str(string).lower()) if word]\n",
    "\n",
    "\n",
    "def get_ngrams(n, word_list):\n",
    "    if n == 1:\n",
    "        return ''.join(word_list)\n",
    "    else:\n",
    "        return [\n",
    "            word[i:i + n]\n",
    "            for word in word_list\n",
    "            for i in range(len(word) - n + 1)\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_n_gram_counter(n, collection):\n",
    "    counter = Counter()\n",
    "    for element in collection:\n",
    "        counter.update(get_ngrams(n, get_words(element)))\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    def __init__(self, elements):\n",
    "        self._ind_to_elem = elements\n",
    "        self._elem_to_ind = {elem: ind for ind, elem in enumerate(elements)}\n",
    "        \n",
    "    def get_elem(self, ind):\n",
    "        return self._ind_to_elem[ind]\n",
    "    \n",
    "    def get_ind(self, elem, default=None):\n",
    "        return self._elem_to_ind.get(elem, default)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._ind_to_elem)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_counter(cls, counter, n_most_common):\n",
    "        return Storage([elem for elem, _ in counter.most_common(n_most_common)])\n",
    "    \n",
    "\n",
    "class Encoder:\n",
    "    def encode(self, example):\n",
    "        raise NotImplemented()\n",
    "\n",
    "    def encode_with_padding(self, examples):\n",
    "        codes = [self.encode(example) for example in examples]\n",
    "        max_len = max(map(len, codes))\n",
    "        return np.array([\n",
    "            np.concatenate([code, np.zeros(max_len - len(code))])\n",
    "            for code in codes\n",
    "        ], dtype=np.int32)\n",
    "\n",
    "\n",
    "class BagOfNgramsEncoder(Encoder):\n",
    "    def __init__(self, collection, ngram_number_array):\n",
    "        self._ngram_storages = [\n",
    "            Storage.from_counter(get_n_gram_counter(n + 1, collection), ngram_number)\n",
    "            for n, ngram_number in enumerate(ngram_number_array)\n",
    "        ]\n",
    "        self._code_size = sum(map(len, self._ngram_storages)) + 2\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return len(self._ngram_storages)\n",
    "    \n",
    "    def _encode_word(self, word):\n",
    "        code = []\n",
    "        if len(word) > 2:\n",
    "            for i in range(len(word) - self.n + 1):\n",
    "                ind = 1\n",
    "                for ngram_len in reversed(range(1, self.n + 1)):\n",
    "                    ngram = word[i:i + ngram_len]\n",
    "                    ngram_ind = self._ngram_storages[ngram_len - 1].get_ind(ngram)\n",
    "                    if ngram_ind is None:\n",
    "                        ind += len(self._ngram_storages[ngram_len - 1])\n",
    "                    else:\n",
    "                        ind += ngram_ind\n",
    "                        code.append(ind)\n",
    "                        break\n",
    "                else:\n",
    "                    code.append(ind)\n",
    "        elif len(word) == 2:\n",
    "            bigram_ind = self._ngram_storages[1].get_ind(word)\n",
    "            if bigram_ind is not None:\n",
    "                return [bigram_ind]\n",
    "            else:\n",
    "                return [\n",
    "                    self._ngram_storages[0].get_ind(word[0], self._code_size - 1),\n",
    "                    self._ngram_storages[0].get_ind(word[1], self._code_size - 1),\n",
    "                ]\n",
    "        elif len(word) == 1:\n",
    "            return [self._ngram_storages[0].get_ind(word, self._code_size - 1)]\n",
    "        return code\n",
    "\n",
    "    def encode(self, string):\n",
    "        return sum((self._encode_word(word) for word in get_words(string)), [])\n",
    "        \n",
    "    @property\n",
    "    def code_size(self):\n",
    "        return self._code_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocEncoder(Encoder):    \n",
    "    def __init__(self, storage, collection, n_words, n_trigrams):\n",
    "        trigram_counter = Counter()\n",
    "        word_counter = Counter()\n",
    "        \n",
    "        for example in tqdm.tqdm(set(collection)):\n",
    "            html = storage.read(BEGINNING + example)\n",
    "            if html is None:\n",
    "                continue\n",
    "            text = get_text(html)\n",
    "            words, trigrams = self._prepare_example(text)\n",
    "            word_counter.update(words)\n",
    "            trigram_counter.update(trigrams)\n",
    "\n",
    "        self._word_storage = Storage.from_counter(word_counter, n_words)\n",
    "        self._trigram_storage = Storage.from_counter(trigram_counter, n_trigrams)    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _prepare_example(example):\n",
    "        words = []\n",
    "        trigrams = []\n",
    "        for example_part in example:\n",
    "            curr_words = get_words(example_part)\n",
    "            words += curr_words\n",
    "            trigrams += get_ngrams(3, curr_words)\n",
    "        return words, trigrams\n",
    "    \n",
    "    def _encode_word(self, word):\n",
    "        word_code = self._word_storage.get_ind(word)\n",
    "        if word_code is None:\n",
    "            return [\n",
    "                1 + len(self._word_storage) +\n",
    "                self._trigram_storage.get_ind(trigram, self._unk_ind - 1 - len(self._word_storage))\n",
    "                for trigram in get_ngrams(3, [word])\n",
    "            ]\n",
    "        else:\n",
    "            return [word_code + 1]\n",
    "    \n",
    "    def encode(self, example):\n",
    "        return sum(\n",
    "            (\n",
    "                self._encode_word(word)\n",
    "                for example_part in example\n",
    "                for word in get_words(example_part)\n",
    "            ), []\n",
    "        )\n",
    "                 \n",
    "    @property\n",
    "    def _unk_ind(self):\n",
    "        return self.code_size - 1\n",
    "                   \n",
    "    @property\n",
    "    def _pad_ind(self):\n",
    "        return 0\n",
    "                                                  \n",
    "    @property\n",
    "    def code_size(self):\n",
    "        return len(self._word_storage) + len(self._trigram_storage) + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это я юзал, когда пытался каждый токен в док-те обрабатывать отдельно. Если тупо засовывать весь документ не хватало памяти на сколько-нибудь адекватную сетку. Поэтому сделал такой костыль: семплируем n подряд идущих токенов из док-та и юзаем в данном батче только их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeWithSampling(Encoder):\n",
    "    def __init__(self, codes_storage, doc_encode_len, code_size):\n",
    "        self._storage = codes_storage\n",
    "        self._doc_encode_len = doc_encode_len\n",
    "        self._code_size = code_size\n",
    "        \n",
    "    def encode(self, doc_name):\n",
    "        code = self._storage.read(doc_name).split(',')\n",
    "        if len(code) > self._doc_encode_len:\n",
    "            ind = np.random.randint(len(code) - self._doc_encode_len + 1)\n",
    "            code = code[ind:ind + self._doc_encode_len]\n",
    "        return list(map(int, code))\n",
    "    \n",
    "    @property\n",
    "    def code_size(self):\n",
    "        return self._code_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем данные на train и val и сохраним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    data = pd.read_csv(path, sep='\\t', header=None).values\n",
    "    return data[:, 0], data[:, 1]\n",
    "\n",
    "\n",
    "def dump(path, *arrays):\n",
    "    data_frame = pd.DataFrame(data=np.transpose(np.array(arrays)))\n",
    "    data_frame.to_csv(path, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "queries, docs = read_data('transliterated_learn.tsv')\n",
    "queries_train, queries_val, docs_train, docs_val = train_test_split(queries, docs, test_size=0.05, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump('train.tsv', queries_train, docs_train)\n",
    "dump('val.tsv', queries_val, docs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train, docs_train = read_data('train.tsv')\n",
    "queries_val, docs_val = read_data('val.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artura rokhama'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/Arthur_Rackham'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем еще отдельно файлы, где есть прям название док-тов, чтобы попытаться обучится только на название док-та не исопльзуя его содержимое (попытка неудачная, конечно же)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_names = [\n",
    "    re.sub('_', ' ', name[6:])\n",
    "    for name in docs_train\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_val_names = [\n",
    "    re.sub('_', ' ', name[6:])\n",
    "    for name in docs_val\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump('train_with_names.tsv', queries_train, doc_train_names)\n",
    "dump('val_with_names.tsv', queries_val, doc_val_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train, doc_train_names = read_data('train_with_names.tsv')\n",
    "queries_val, doc_val_names = read_data('val_with_names.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_encoder = BagOfNgramsEncoder(queries_train, [600, 2500, 20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word_num = 65000\n",
    "doc_trigram_num = 10000\n",
    "doc_code_size = doc_word_num + doc_trigram_num + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_encoder = DocEncoder(file_storage, docs_train, doc_word_num, doc_trigram_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним кодировщик док-тов, потому что строится он долго"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('big_big_doc_encoder.pkl', 'wb') as doc_encoder_file:\n",
    "    pickle.dump(doc_encoder, doc_encoder_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('big_big_doc_encoder.pkl', 'rb') as doc_encoder_file:\n",
    "    doc_encoder = pickle.load(doc_encoder_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_encoder.code_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним так же и сами коды док-тов, они тоже строятся долго"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = FileStorage('encoded_docs', need_compression=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in tqdm.tqdm(set(np.concatenate([docs_train, docs_val]))):\n",
    "    if doc not in encoded_docs and BEGINNING + doc in file_storage:\n",
    "        code = doc_encoder.encode(get_text(file_storage.read(BEGINNING + doc)))\n",
    "        encoded_docs.write(doc, ','.join(map(str, code)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала делал так:\n",
    "Закодированные запросы и док-ты подаю в керасовский Embeding, а потом прогоняю через lstm. Но с док-тами это сделать не получались из-за их размеров. Перешел на GlobalPooling, но все равно все было оч медленно, даже с EncodeWithSampling. Да и глупо как-то, ведь если сделать bag of words и от него денс, это по факту тоже самое, что Embdeing + Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiled(x_positive, y_positive, y_negative, batch_size, negative_cnt):\n",
    "    return [\n",
    "        np.tile(x_positive, (1 + negative_cnt, 1)),\n",
    "        np.concatenate([y_positive, np.repeat(y_negative, batch_size, axis=0)], axis=0),\n",
    "    ]\n",
    "\n",
    "\n",
    "def batch_generator(queries, doc_names, doc_encoder, query_encoder, batch_size, negative_cnt=2):\n",
    "    allowed_inds = [ind for ind, name in enumerate(doc_names) if name in doc_encoder._storage]\n",
    "    while True:\n",
    "        try:\n",
    "            indexes = np.random.choice(allowed_inds, batch_size + negative_cnt, replace=False)\n",
    "            batch_indexes = indexes[:batch_size]\n",
    "            query_codes = query_encoder.encode_with_padding(queries[indexes])\n",
    "            positive_query_codes = query_codes[:batch_size]\n",
    "            negative_query_codes = query_codes[batch_size:]\n",
    "            doc_codes = doc_encoder.encode_with_padding(doc_names[batch_indexes])\n",
    "            if len(np.where(doc_codes >= doc_encoder.code_size)[0]) == 0:\n",
    "                yield (\n",
    "                    get_tiled(doc_codes, positive_query_codes, negative_query_codes, batch_size, negative_cnt),\n",
    "                    np.concatenate([np.ones(batch_size), np.zeros(batch_size * negative_cnt)]),\n",
    "                )\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense(units, activation='relu'):\n",
    "    return keras.layers.Dense(\n",
    "        units, activation=activation, kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_embed(embed_layers, data):\n",
    "    for layer in embed_layers:\n",
    "        data = layer(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = keras.layers.Input(shape=(None,), dtype='int32')\n",
    "query = keras.layers.Input(shape=(None,), dtype='int32')\n",
    "\n",
    "doc_embed_layers = [\n",
    "    keras.layers.Embedding(doc_encoder_with_sampling.code_size, 128),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "query_embed_layers = [\n",
    "    keras.layers.Embedding(query_encoder.code_size, 128),\n",
    "    keras.layers.LSTM(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "doc_embed = get_embed(doc_embed_layers, doc)\n",
    "query_embed = get_embed(query_embed_layers, query)\n",
    "\n",
    "similarity = keras.layers.Dot(axes=1, normalize=True)([doc_embed, query_embed])\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[doc, query],\n",
    "    outputs=similarity,\n",
    ")\n",
    "\n",
    "model.compile(keras.optimizers.Adam(lr=1e-4), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "negative_cnt = 2\n",
    "\n",
    "training_history = model.fit_generator(\n",
    "    generator=batch_generator(\n",
    "        queries_train, docs_train, doc_encoder_with_sampling,\n",
    "        query_encoder, batch_size, negative_cnt\n",
    "    ),\n",
    "    epochs=50,\n",
    "    steps_per_epoch=1000,\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(batch_size=batch_size),\n",
    "        keras.callbacks.ModelCheckpoint('first_dssm.bin', monitor='val_loss', save_best_only=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=3, min_delta=0.0002),\n",
    "    ],\n",
    "    validation_data=batch_generator(\n",
    "        queries_val, docs_val, doc_encoder_with_sampling,\n",
    "        query_encoder, batch_size, negative_cnt\n",
    "    ),\n",
    "    validation_steps=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше перехожу собственно к bag of words для документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_loss(similarity, labels):\n",
    "    return 1 - keras.backend.mean(similarity * labels)\n",
    "\n",
    "\n",
    "def mean_negative_score(score, labels):\n",
    "    return keras.backend.sum(score * (1 - labels)) / keras.backend.sum(1 - labels)\n",
    "\n",
    "\n",
    "def mean_positive_score(score, labels):\n",
    "    return keras.backend.sum(score * labels) / keras.backend.sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_docs(encoded_docs, doc_code_size, doc_names):\n",
    "    doc_data = []\n",
    "    doc_col_indexes = []\n",
    "    doc_indptr = []\n",
    "    for doc in doc_names:            \n",
    "        doc_counter = Counter([\n",
    "            int(code) % doc_code_size\n",
    "            for code in encoded_docs.read(doc).split(',') if code\n",
    "        ])\n",
    "        doc_indptr.append(len(doc_col_indexes))\n",
    "        for token_code, cnt in doc_counter.items():\n",
    "            doc_data.append(cnt)\n",
    "            doc_col_indexes.append(token_code)\n",
    "    doc_indptr.append(len(doc_col_indexes))\n",
    "    return csr_matrix(\n",
    "        (doc_data, doc_col_indexes, doc_indptr), shape=(len(doc_names), doc_code_size)\n",
    "    )\n",
    "\n",
    "\n",
    "def sparse_docs_batch_generator(\n",
    "        queries, doc_names, encoded_docs, query_encoder, batch_size, doc_code_size):\n",
    "    allowed_inds = [ind for ind, name in enumerate(doc_names) if name in encoded_docs]\n",
    "    while True:\n",
    "        positive_cnt = batch_size // 2\n",
    "        negative_cnt = batch_size - positive_cnt\n",
    "        indexes = np.random.choice(allowed_inds, positive_cnt + 2 * negative_cnt, replace=False)\n",
    "        query_indexes = np.concatenate([indexes[:positive_cnt], indexes[batch_size:]])\n",
    "        query_codes = query_encoder.encode_with_padding(queries[query_indexes])\n",
    "\n",
    "        yield (\n",
    "            [get_sparse_docs(encoded_docs, doc_code_size, doc_names[indexes[:batch_size]]), query_codes],\n",
    "            np.concatenate([np.ones(positive_cnt), np.zeros(negative_cnt)]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Думал сделать negative sampling, мб и сделаю когда-то потом, но из-за проблем ниже не дошли до него руки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingGenerator:\n",
    "    def __init__(\n",
    "            self, queries, doc_names, encoded_docs, query_encoder,\n",
    "            half_batch_size, doc_code_size, initial_negative_cnt):\n",
    "        self._queries = queries\n",
    "        self._doc_names = doc_names\n",
    "        self._encoded_docs = encoded_docs\n",
    "        self._query_encoder = query_encoder\n",
    "        self._half_batch_size = half_batch_size\n",
    "        self._doc_code_size = doc_code_size\n",
    "        self._initial_negative_cnt = initial_negative_cnt\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пытаюсь значит чего-то обучить и получается плохо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = keras.layers.Input(shape=(doc_code_size,), sparse=True)\n",
    "query = keras.layers.Input(shape=(None,), dtype='int32')\n",
    "\n",
    "doc_embed_layers = [\n",
    "    get_dense(256),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "query_embed_layers = [\n",
    "    keras.layers.Embedding(query_encoder.code_size, 128),\n",
    "    keras.layers.LSTM(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "doc_embed = get_embed(doc_embed_layers, doc)\n",
    "query_embed = get_embed(query_embed_layers, query)\n",
    "\n",
    "similarity = keras.layers.Dot(axes=1, normalize=True)([doc_embed, query_embed])\n",
    "score = keras.layers.Activation('sigmoid')(similarity)\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[doc, query],\n",
    "    outputs=score,\n",
    ")\n",
    "\n",
    "model.compile(keras.optimizers.Adam(lr=1e-2), loss='binary_crossentropy', metrics=[\n",
    "    mean_positive_score, mean_negative_score,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 517s 258ms/step - loss: 0.7122 - mean_positive_score: 0.5000 - mean_negative_score: 0.5000 - val_loss: 0.7091 - val_mean_positive_score: 0.5000 - val_mean_negative_score: 0.5000\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 540s 270ms/step - loss: 0.7100 - mean_positive_score: 0.5000 - mean_negative_score: 0.5000 - val_loss: 0.8002 - val_mean_positive_score: 0.5000 - val_mean_negative_score: 0.5000\n",
      "Epoch 3/20\n",
      " 516/2000 [======>.......................] - ETA: 6:19 - loss: 0.7307 - mean_positive_score: 0.5000 - mean_negative_score: 0.5000"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "training_history = model.fit_generator(\n",
    "    generator=sparse_docs_batch_generator(\n",
    "        queries_train, docs_train, encoded_docs,\n",
    "        query_encoder, batch_size, doc_code_size,\n",
    "    ),\n",
    "    epochs=20,\n",
    "    steps_per_epoch=2000,\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(batch_size=batch_size),\n",
    "        keras.callbacks.ModelCheckpoint('doc_bow_dssm.bin', monitor='val_loss', save_best_only=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5, min_delta=0.0002),\n",
    "    ],\n",
    "    validation_data=sparse_docs_batch_generator(\n",
    "        queries_val, docs_val, encoded_docs,\n",
    "        query_encoder, batch_size, doc_code_size,\n",
    "    ),\n",
    "    validation_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_encoder.encode('The silence of the Somme: Sound and realism in British and Dutch poems mediating The Battle of the Somme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_encoder.encode_with_padding([\n",
    "        'The silence of the Somme: Sound and realism in British '\n",
    "        'and Dutch poems mediating The Battle of the Somme',\n",
    "        'runescape'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(sparse_docs_batch_generator(\n",
    "        queries_train, docs_train, encoded_docs,\n",
    "        query_encoder, batch_size, doc_code_size,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148],\n",
       "       [0.48224148]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\n",
    "    get_sparse_docs(encoded_docs, doc_code_size, [\n",
    "        '/wiki/Battle_of_the_Somme',\n",
    "        '/wiki/RuneScape',\n",
    "        '/wiki/Liliaceae',\n",
    "        '/wiki/Annual_percentage_rate'\n",
    "    ]),\n",
    "    query_encoder.encode_with_padding([\n",
    "        'the silence of the somme: sound and realism in british '\n",
    "        'and dutch poems mediating the battle of the somme',\n",
    "        'runescape',\n",
    "        'liliaceae - tulipa lord beaconsfield + parmesiano',\n",
    "        'one-tenth of the amount to payday max apr if you borrow one hundred '\n",
    "        'dollars in a fourteen-day period the highest yearly percentage rate '\n",
    "        'could be 309 max amount there isn t any specified amount of',\n",
    "    ]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробовал много чего: менять lr, брать больше слов при энкодинге, делать сетку глубже/шире, другой лосс (linear_loss), менять соотношеине позитивов и негативов в батче. Ничего не помогло - сеть во всех случаях сходилась к константым предсказаниям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше тут попытался использовать только имена документов, но разумеется это тоже ни к чему не привело"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_name_batch_generator(queries, doc_names, encoder, batch_size, negative_cnt):\n",
    "    allowed_inds = np.arange(len(doc_names))\n",
    "    while True:\n",
    "        positive_cnt = batch_size // 2\n",
    "        negative_cnt = batch_size - positive_cnt\n",
    "        indexes = np.random.choice(allowed_inds, positive_cnt + 2 * negative_cnt, replace=False)\n",
    "        query_indexes = np.concatenate([indexes[:positive_cnt], indexes[batch_size:]])\n",
    "        query_codes = encoder.encode_with_padding(queries[query_indexes])\n",
    "        doc_name_codes = encoder.encode_with_padding(doc_names[indexes[:batch_size]])\n",
    "        yield (\n",
    "            [doc_name_codes, query_codes],\n",
    "            np.concatenate([np.ones(positive_cnt), np.zeros(negative_cnt)]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "doc_name = keras.layers.Input(shape=(None,), dtype='int32')\n",
    "query = keras.layers.Input(shape=(None,), dtype='int32')\n",
    "\n",
    "doc_embed_layers = [\n",
    "    keras.layers.Embedding(query_encoder.code_size, 128),\n",
    "    keras.layers.LSTM(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "query_embed_layers = [\n",
    "    keras.layers.Embedding(query_encoder.code_size, 128),\n",
    "    keras.layers.LSTM(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    get_dense(128, activation=None),\n",
    "]\n",
    "\n",
    "doc_embed = get_embed(doc_embed_layers, doc_name)\n",
    "query_embed = get_embed(query_embed_layers, query)\n",
    "\n",
    "similarity = keras.layers.Dot(axes=1, normalize=True)([doc_embed, query_embed])\n",
    "score = keras.layers.Activation('sigmoid')(similarity)\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[doc_name, query],\n",
    "    outputs=score,\n",
    ")\n",
    "\n",
    "model.compile(keras.optimizers.Adam(), loss='binary_crossentropy', metrics=[\n",
    "    mean_positive_score, mean_negative_score,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 344s 172ms/step - loss: 0.8135 - mean_positive_score: 0.5000 - mean_negative_score: 0.5000 - val_loss: 0.6932 - val_mean_positive_score: 0.5000 - val_mean_negative_score: 0.5000\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 327s 164ms/step - loss: 0.6933 - mean_positive_score: 0.5000 - mean_negative_score: 0.5000 - val_loss: 0.6932 - val_mean_positive_score: 0.5000 - val_mean_negative_score: 0.5000\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 316s 158ms/step - loss: 0.6932 - mean_positive_score: 0.5000 - mean_negative_score: 0.5000 - val_loss: 0.6932 - val_mean_positive_score: 0.5000 - val_mean_negative_score: 0.5000\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 333s 166ms/step - loss: 0.6932 - mean_positive_score: 0.5000 - mean_negative_score: 0.5000 - val_loss: 0.6932 - val_mean_positive_score: 0.5000 - val_mean_negative_score: 0.5000\n",
      "Epoch 5/20\n",
      "1578/2000 [======================>.......] - ETA: 1:00 - loss: 0.6932 - mean_positive_score: 0.5000 - mean_negative_score: 0.5000"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[32,13633,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training/Adam/gradients/lstm_3/transpose_grad/transpose}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-bf486705cb9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mquery_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_cnt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     ),\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,13633,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training/Adam/gradients/lstm_3/transpose_grad/transpose}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "negative_cnt = 16\n",
    "\n",
    "training_history = model.fit_generator(\n",
    "    generator=doc_name_batch_generator(\n",
    "        queries_train, doc_train_names,\n",
    "        query_encoder, batch_size, negative_cnt,\n",
    "    ),\n",
    "    epochs=20,\n",
    "    steps_per_epoch=2000,\n",
    "    callbacks=[\n",
    "        keras.callbacks.TensorBoard(batch_size=batch_size),\n",
    "        keras.callbacks.ModelCheckpoint('doc_bow_dssm.bin', monitor='val_loss', save_best_only=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5, min_delta=0.0002),\n",
    "    ],\n",
    "    validation_data=doc_name_batch_generator(\n",
    "        queries_val, doc_val_names,\n",
    "        query_encoder, batch_size, negative_cnt,\n",
    "    ),\n",
    "    validation_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более опытные товарищи подсказали, что такая фигня иногда случается и что, например, добавление словестных биграм может помочь. В общем я еще попытаюсь тут что-то сделать, но пока вот так("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
