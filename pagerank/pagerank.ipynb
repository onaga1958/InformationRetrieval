{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_storage import FileStorage\n",
    "from collections import defaultdict\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = FileStorage('../storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_storage = FileStorage('../filtered_storage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлифльтруем сторадж, полученный в прошлом задании"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginning = 'https://simple.wikipedia.org/wiki/'\n",
    "ban_patterns = [\n",
    "    'Help', 'Help_talk', 'Wikipedia', 'Special', 'File', 'Template', 'Talk',\n",
    "    'Template_talk', 'User_talk', 'User', 'Meta', 'user', 'MediaWiki', 'MediaWiki_talk',\n",
    "    'Wikipedia_talk', 'Category_talk', 'Module', 'Media', 'Category', \n",
    "]\n",
    "\n",
    "def filter_url(url):\n",
    "    url_end = url[len(beginning):]\n",
    "    return any(url_end.startswith(ban_pattern + ':') for ban_pattern in ban_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url, page in storage.items():\n",
    "    if not filter_url(url):\n",
    "        filtered_storage.write(url, page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сторим соседей для каждого урла: извлекаем другие урлы из html, применяем к ним urldefrag и проверяем, что url находится в storage, то есть принадлежит к нашему графу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urldefrag\n",
    "\n",
    "def extract_links_from_html(url, html):\n",
    "    parser = BeautifulSoup(html)\n",
    "    return [\n",
    "        urldefrag(urljoin(url, link.get('href'))).url\n",
    "        for link in parser.findAll('a')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neighbors(storage):\n",
    "    result = {}\n",
    "    for url, page in tqdm.tqdm(storage.items()):\n",
    "        result[url] = [\n",
    "            link for link in extract_links_from_html(url, page)\n",
    "            if link in storage\n",
    "        ]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normilize_pagerank(pagerank):\n",
    "    max_rank = max(pagerank.values())\n",
    "    return {url: rank / max_rank for url, rank in pagerank.items()}\n",
    "\n",
    "\n",
    "def calc_pagerank(storage, iterations=20, delta=0.1, prev_pagerank=None, neighbors=None):\n",
    "    if prev_pagerank is None:\n",
    "        prev_pagerank = defaultdict(lambda: 1 / len(storage))\n",
    "    if neighbors is None:\n",
    "        neighbors = build_neighbors(storage)\n",
    "\n",
    "    for ind in range(iterations):\n",
    "        print('{} iteration, India rank: {}'.format(\n",
    "            ind, prev_pagerank['https://simple.wikipedia.org/wiki/India']\n",
    "        ))\n",
    "        pagerank = defaultdict(lambda: delta)\n",
    "        for url in storage.keys():\n",
    "            for neighbor_url in neighbors[url]:\n",
    "                pagerank[neighbor_url] += prev_pagerank[url] / len(neighbors) * (1 - delta)\n",
    "        prev_pagerank = normilize_pagerank(pagerank)\n",
    "    return prev_pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "151127it [1:00:03, 41.94it/s]\n"
     ]
    }
   ],
   "source": [
    "neighbors = build_neighbors(filtered_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('neighbors.pkl', 'wb') as f_out:\n",
    "    pickle.dump(neighbors, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration, India rank: 6.61695130585534e-06\n",
      "1 iteration, India rank: 0.9998837565569197\n",
      "2 iteration, India rank: 0.07561958666173003\n",
      "3 iteration, India rank: 0.5244410349795183\n",
      "4 iteration, India rank: 0.11969777718198288\n",
      "5 iteration, India rank: 0.37737983824209537\n",
      "6 iteration, India rank: 0.1541345572981528\n",
      "7 iteration, India rank: 0.311416860969089\n",
      "8 iteration, India rank: 0.17921224047126483\n",
      "9 iteration, India rank: 0.2770313844149672\n",
      "10 iteration, India rank: 0.19655436199770773\n",
      "11 iteration, India rank: 0.2577005784515863\n",
      "12 iteration, India rank: 0.20812264917378712\n",
      "13 iteration, India rank: 0.24636982801133658\n",
      "14 iteration, India rank: 0.21565508353137788\n",
      "15 iteration, India rank: 0.23956519885937994\n",
      "16 iteration, India rank: 0.2204827325576716\n",
      "17 iteration, India rank: 0.2354190248469218\n",
      "18 iteration, India rank: 0.22354556369840317\n",
      "19 iteration, India rank: 0.2328703364303573\n"
     ]
    }
   ],
   "source": [
    "pagerank = calc_pagerank(filtered_storage, neighbors=neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration, India rank: 0.22547622228433256\n",
      "1 iteration, India rank: 0.23129514282086075\n",
      "2 iteration, India rank: 0.22668826437635\n",
      "3 iteration, India rank: 0.2303183537684309\n",
      "4 iteration, India rank: 0.22744722242311402\n",
      "5 iteration, India rank: 0.2297113854935061\n",
      "6 iteration, India rank: 0.22792170590075722\n",
      "7 iteration, India rank: 0.22933373572829416\n",
      "8 iteration, India rank: 0.22821804482450017\n",
      "9 iteration, India rank: 0.22909857783823864\n",
      "10 iteration, India rank: 0.22840300751052484\n",
      "11 iteration, India rank: 0.228952075013358\n",
      "12 iteration, India rank: 0.22851840852999267\n",
      "13 iteration, India rank: 0.2288607757935349\n",
      "14 iteration, India rank: 0.22859039139919576\n",
      "15 iteration, India rank: 0.22880386796403082\n",
      "16 iteration, India rank: 0.22863528480277076\n",
      "17 iteration, India rank: 0.2287683924034312\n",
      "18 iteration, India rank: 0.2286632807176408\n",
      "19 iteration, India rank: 0.2287462757655117\n"
     ]
    }
   ],
   "source": [
    "pagerank = calc_pagerank(filtered_storage, neighbors=neighbors, prev_pagerank=pagerank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсортируем и сохраним результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pagerank = sorted(pagerank.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0   1.0 https://simple.wikipedia.org/wiki/Main_Page\n",
      "  1 0.3121 https://simple.wikipedia.org/wiki/United_States\n",
      "  2 0.3093 https://simple.wikipedia.org/wiki/International_Standard_Book_Number\n",
      "  3 0.2784 https://simple.wikipedia.org/wiki/France\n",
      "  4 0.2545 https://simple.wikipedia.org/wiki/Germany\n",
      "  5 0.2526 https://simple.wikipedia.org/wiki/List_of_Wikipedias\n",
      "  6 0.2502 https://simple.wikipedia.org/wiki/United_Kingdom\n",
      "  7 0.2454 https://simple.wikipedia.org/wiki/Japan\n",
      "  8 0.2437 https://simple.wikipedia.org/wiki/Canada\n",
      "  9 0.2429 https://simple.wikipedia.org/wiki/Italy\n",
      " 10 0.2406 https://simple.wikipedia.org/wiki/Geographic_coordinate_system\n",
      " 11 0.2393 https://simple.wikipedia.org/wiki/England\n",
      " 12 0.2366 https://simple.wikipedia.org/wiki/Americans\n",
      " 13 0.236 https://simple.wikipedia.org/wiki/Spain\n",
      " 14 0.2359 https://simple.wikipedia.org/wiki/Departments_of_France\n",
      " 15 0.233 https://simple.wikipedia.org/wiki/Digital_object_identifier\n",
      " 16 0.2325 https://simple.wikipedia.org/wiki/Communes_of_France\n",
      " 17 0.2321 https://simple.wikipedia.org/wiki/IMDb\n",
      " 18 0.2316 https://simple.wikipedia.org/wiki/Australia\n",
      " 19 0.231 https://simple.wikipedia.org/wiki/Time_zone\n"
     ]
    }
   ],
   "source": [
    "for ind, (url, rank) in enumerate(sorted_pagerank):\n",
    "    print(\"{:>3} {:>5.4} {}\".format(ind, rank, url))\n",
    "    if ind == 19:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pagerank_results.txt', 'w') as f_out:\n",
    "    for ind, (url, rank) in enumerate(sorted_pagerank):\n",
    "        f_out.write(\"{:>6} {:>7.6} {}\\n\".format(ind + 1, rank, url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
